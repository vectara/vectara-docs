---
id: create-chat-completion
title: "Creates a model response for the given chat conversation"
description: "The Chat Completions API provides an OpenAI-compatible interface for generating model responses in multi-turn chat conversations. This API enables you to integrate our language models directly into applications designed to work with the OpenAI Chat Completions format, making it easy to leverage Vectara capabilities with minimal changes to existing tools or code."
sidebar_label: "Creates a model response for the given chat conversation"
hide_title: true
hide_table_of_contents: true
api: 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
sidebar_class_name: "post api-method"
info_path: docs/rest-api/vectara-rest-api-v-2
custom_edit_url: null
---

import MethodEndpoint from "@theme/ApiExplorer/MethodEndpoint";
import ParamsDetails from "@theme/ParamsDetails";
import RequestSchema from "@theme/RequestSchema";
import StatusCodes from "@theme/StatusCodes";
import OperationTabs from "@theme/OperationTabs";
import TabItem from "@theme/TabItem";
import Heading from "@theme/Heading";

<Heading
  as={"h1"}
  className={"openapi__heading"}
  children={"Creates a model response for the given chat conversation"}
>
</Heading>

<MethodEndpoint
  method={"post"}
  path={"/v2/llms/chat/completions"}
  context={"endpoint"}
>
  
</MethodEndpoint>



The Chat Completions API provides an OpenAI-compatible interface for generating model responses in multi-turn chat conversations. This API enables you to integrate our language models directly into applications designed to work with the OpenAI Chat Completions format, making it easy to leverage Vectara capabilities with minimal changes to existing tools or code.

Use this API to enable interactive chat experiences that support context-aware responses, streaming output, and token usage tracking.

The request includes a series of chat messages and optional parameters that control the behavior and structure of the model response. The request body must include the `messages` parameter, an array of message objects (role, content) representing the full conversation so far.

### Streaming responses

If the `stream` parameter is set to `true`, the response appears as a series of text/event-stream parts (also known as chunks). Each chunk includes a `delta` field showing the incremental message update.

### Example request

This example sends a simple chat conversation to the API, asking the assistant for the capital of France. The request includes a system prompt, a user message, and a temperature setting for response variability.
```json
{
  "model": "chat-model-001","messages": [{ "role": "system", "content": "You are a helpful assistant." },
  { "role": "user", "content": "What is the capital of France?" }
],
"temperature": 0.7,
"stream": false
}
```

### Example response
The response includes a generated reply from the assistant, along with token usage statistics. In this example, the model returns a direct answer to a user question.
```json
{
"id": "chatcmpl-abc123",}
"object": "chat.completion",
"created": 1712454830,
"model": "chat-model-001",
"choices": [
  {
    "index": 0,
    "message": {
      "role": "assistant",
      "content": "The capital of France is Paris."
  },
    "finish_reason": "stop"
  }
],
"usage": {
  "prompt_tokens": 21,
  "completion_tokens": 9,
  "total_tokens": 30
  } 
} 
```
If the input summary is accurate, the `corrected_summary` matches the `original_summary`.


<Heading
  id={"request"}
  as={"h2"}
  className={"openapi-tabs__heading"}
  children={"Request"}
>
</Heading>

<ParamsDetails
  parameters={[{"in":"header","name":"Request-Timeout","description":"The API will make a best effort to complete the request in the specified seconds or time out.","schema":{"type":"integer","minimum":1},"required":false},{"in":"header","name":"Request-Timeout-Millis","description":"The API will make a best effort to complete the request in the specified milliseconds or time out.","schema":{"type":"integer","minimum":1}}]}
>
  
</ParamsDetails>

<RequestSchema
  title={"Body"}
  body={{"required":true,"content":{"application/json":{"schema":{"description":"The request object for creating a chat completion with an OpenAI-compatible interface. This object is compatible with OpenAI's chat completion schema and supports customizable parameters for response generation.","type":"object","required":["messages","model"],"properties":{"model":{"type":"string","description":"The ID of the model to use. This field is required."},"messages":{"type":"array","description":"An ordered array of messages that represent the full context of the conversation to date. Each message includes a `role` and `content`.","minItems":1,"items":{"description":"A message in the chat completion request, representing part of the conversation.","type":"object","required":["role","content"],"properties":{"role":{"type":"string","description":"The role of the author of this message. Common values include 'system', 'user', 'assistant', 'function', and 'tool'."},"content":{"description":"The contents of the message. Follows the exact format of the content field in the OpenAI Chat Completions API (https://platform.openai.com/docs/api-reference/chat/create). Can be a string for text-only messages, or an array of content parts for multimodal messages with text and images."},"name":{"type":"string","description":"The name of the author of this message, used to connect messages in a conversation."}},"title":"ChatCompletionRequestMessage"}},"stream":{"type":"boolean","description":"Optional. When set to `true`, the API streams partial message deltas as they become available, similar to ChatGPT's streaming mode.","default":false}},"additionalProperties":true,"title":"CreateChatCompletionRequest"}}}}}
>
  
</RequestSchema>

<StatusCodes
  id={undefined}
  label={undefined}
  responses={{"200":{"description":"A chat completion","content":{"application/json":{"schema":{"description":"Response object containing the generated chat completion.","type":"object","required":["object","choices"],"properties":{"object":{"type":"string","enum":["chat.completion"],"description":"The object type, which is always 'chat.completion'."},"choices":{"type":"array","description":"A list of chat completion choices. Can be more than one if `n` is greater than `1`.","items":{"description":"A single chat completion choice returned in the response","type":"object","required":["index","message"],"properties":{"index":{"type":"integer","description":"The index of the choice in the array of choices."},"message":{"description":"A message in the chat completion response from the assistant.","type":"object","required":["role"],"properties":{"role":{"type":"string","description":"The role of the author of this message, typically 'assistant' for responses."}},"additionalProperties":true,"title":"ChatCompletionResponseMessage"}},"additionalProperties":true,"title":"ChatCompletionResponseChoice"}}},"additionalProperties":true,"title":"CreateChatCompletionResponse"}},"text/event-stream":{"schema":{"description":"The response object for streaming chat completions in chunks.","type":"object","required":["object","choices"],"properties":{"object":{"type":"string","enum":["chat.completion.chunk"],"description":"The object type, which is always 'chat.completion.chunk' for streaming responses."},"choices":{"type":"array","description":"A list of chat completion chunks.","items":{"description":"A single chat completion chunk choice in a streaming response.","type":"object","required":["index","delta"],"properties":{"index":{"type":"integer","description":"The index of the choice in the array of choices."},"delta":{"description":"A partial message update to be merged with previous chunks in a streaming response.","type":"object","properties":{"role":{"type":"string","description":"The role of the author of this message, typically 'assistant' for responses."}},"additionalProperties":true,"title":"ChatCompletionStreamResponseDelta"}},"additionalProperties":true,"title":"ChatCompletionStreamResponseChoice"}}},"additionalProperties":true,"title":"CreateChatCompletionStreamResponse"}}}},"400":{"description":"Chat completion request was malformed.","content":{"application/json":{"schema":{"description":"Error returned when a request contains invalid parameters or violates schema validation.","type":"object","properties":{"field_errors":{"description":"The errors that relate to specific fields in the request.","type":"object","additionalProperties":{"type":"string"}},"messages":{"type":"array","items":{"title":"message","type":"string"}},"request_id":{"description":"The ID of the request that can be used to help Vectara support debug what went wrong.","type":"string"}},"title":"BadRequestError"}}}},"403":{"description":"Permissions do not allow creating a chat completion.","content":{"application/json":{"schema":{"description":"A general error response with an error code and message.","type":"object","properties":{"messages":{"description":"The messages describing why the error occurred.","type":"array","items":{"title":"message","type":"string","example":"Internal server error."}},"request_id":{"description":"The ID of the request that can be used to help Vectara support debug what went wrong.","type":"string"}},"title":"Error"}}}}}}
>
  
</StatusCodes>


      