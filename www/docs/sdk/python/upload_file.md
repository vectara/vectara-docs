---
id: upload
title: Upload Files
hide_table_of_contents: true
---

import CodePanel from '@site/src/theme/CodePanel';
import { Spacer } from "@site/src/components/ui/Spacer";

This guide demonstrates how to upload files (PDFs, DOCX, and more) to a 
Vectara corpus using the Python SDK. Uploaded files are automatically parsed, 
chunked, and indexedâ€”making their contents instantly available for search 
and Retrieval Augmented Generation (RAG).

Use file upload for:
- Bulk onboarding of policy docs, technical manuals, invoices, contracts, or 
  research papers
- Ingesting new content as soon as it is generated by your business
- Processing documents with tables, charts, and structured data

:::info Prerequisites
This guide assumes you have a corpus called `my-docs`. If you haven't created a corpus yet, follow the [Quick Start](/docs/sdk/python/python-quickstart) guide to set up your first corpus.
:::

## Prerequisites

<CodePanel
  title="Install Vectara SDK"
  snippets={[
    { language: 'bash', code: `pip install vectara` }
  ]}
  customWidth="50%"
/>

**Setup Requirements:**
1. **Install the SDK** with `pip install vectara`
2. **Get an API key** from the [Vectara Console](https://console.vectara.com)
3. **Create a corpus** with `client.corpora.create()` (see [Corpus Management](https://docs.vectara.com/docs/api-reference/indexing-apis/corpus))
4. **Download sample files** or prepare your own files (PDFs, DOCX, etc.)

**Sample Files for Testing:**
- [Employee Handbook (TXT)](../../../static/sample-files/sample-employee-handbook.txt) - Company policies and procedures
- [Security Policy (TXT)](../../../static/sample-files/sample-security-policy.txt) - Information security guidelines  
- [API Documentation (TXT)](../../../static/sample-files/sample-api-documentation.txt) - Technical documentation example

<Spacer size="l" />

---

## Initialize the Vectara Client

<CodePanel
  title="Initialize Vectara Client"
  snippets={[
    {
      language: 'python',
      code: `from vectara import Vectara
from vectara.core.api_error import ApiError

# Initialize client with API key
client = Vectara(api_key="YOUR_API_KEY")`
    }
  ]}
  annotations={{
    python: [
      { line: 5, text: 'Use an API key with indexing permissions for file uploads' }
    ]
  }}
  customWidth="50%"
/>

Set up authentication to securely access file upload capabilities. Ensure your 
API key has indexing (write) permissions for the target corpus.

<Spacer size="l" />
<Spacer size="l" />
<Spacer size="l" />
<Spacer size="l" />

---

## Upload a basic file

<CodePanel
  title="Upload sample employee handbook"
  snippets={[
    {
      language: 'python',
      code: `filename = "sample-employee-handbook.txt"
with open(filename, "rb") as f:
    file_content = f.read()

response = client.upload.file(
    corpus_key="my-docs",
    file=file_content,
    filename=filename,
    metadata={
        "document_type": "handbook",
        "department": "hr", 
        "year": "2025",
        "category": "policies"
    }
)`
    }
  ]}
  annotations={{
    python: [
      { line: 2, text: 'Open file in binary mode for upload' },
      { line: 6, text: 'Upload to your corpus' },
      { line: 9, text: 'Include descriptive metadata for filtering and search' },
    ]
  }}
  customWidth="50%"
/>

Upload a document (employee handbook or policy document) to your corpus. Vectara 
automatically parses, chunks, and indexes the content for semantic search. No 
manual processing is required.

The `upload.file` method corresponds to the HTTP POST `/v2/upload_file` 
endpoint. For more details on request and response parameters, see the 
[Upload File REST API](https://docs.vectara.com/docs/rest-api/upload-file).

**Key Parameters:**
- `corpus_key`: Target corpus identifier where the file will be stored
- `file`: Binary content of the file (read with `"rb"` mode)
- `filename`: Name of the file being uploaded
- `metadata`: Optional key-value pairs for filtering and categorization

**Supported File Types:** PDF, DOCX, DOC, TXT, HTML, Markdown

:::tip Note
Each file uploaded can be up to 10 MB in size.
:::

To update or overwrite an existing file, you must first delete the document 
using `client.documents.delete()` and then re-upload it, as direct updates to 
content are not supported. The file name is used as the document ID. Attempting 
to upload a file with the same name but different content will result in a 409 
error.

**Error Handling:**
- **400 Bad Request**: Invalid parameters or unsupported file type
- **403 Forbidden**: Insufficient permissions
- **404 Not Found**: Corpus not found
- **409 Conflict**: Document with same ID exists but different content
- **413 Payload Too Large**: File exceeds size limit

<Spacer size="l" />

---

## Upload with table extraction

<CodePanel
  title="Upload technical document with table extraction"
  snippets={[
    {
      language: 'python',
      code: `filename = "sample-api-documentation.txt"
with open(filename, "rb") as f:
    file_content = f.read()

response = client.upload.file(
    corpus_key="my-docs",
    file=file_content,
    filename=filename,
    metadata={
        "document_type": "technical_documentation",
        "category": "api",
        "year": "2025", 
        "contains_tables": True,
        "department": "engineering"
    },
    table_extraction_config={"extract_tables": True},
    chunking_strategy={"type": "sentence_chunking_strategy"}
)`
    }
  ]}
  annotations={{
    python: [
      { line: 6, text: 'Specify the corpus key of where to upload the document' },
      { line: 16, text: 'Enable table extraction for structured data processing' },
      { line: 17, text: 'Configure chunking strategy for optimal text segmentation' },
      { line: 13, text: 'Mark documents that contain tabular data' }
    ]
  }}
  customWidth="50%"
/>

Upload documents containing tables, charts, or structured data with enhanced extraction 
capabilities. Perfect for technical documentation, API references, or any document 
with tabular information.

**Table Extraction Benefits:**
- Automatically extracts and indexes table content
- Makes tabular data searchable alongside text content
- Preserves table structure and relationships
- Enables queries about specific data points within tables

**Use Cases:**
- Technical specifications with parameter tables
- API documentation with endpoint tables
- Research papers with data tables
- Configuration guides with settings tables

:::tip Note
Each file uploaded can be up to 10 MB in size.
:::

To update or overwrite an existing file, you must first delete the document 
using `client.documents.delete()` and then re-upload it, as direct updates to 
content are not supported. The file name is used as the document ID. 
Attempting to upload a file with the same name but different content will 
result in a 409 error.

**Error Handling:**
- **400 Bad Request**: Invalid parameters or unsupported file type
- **403 Forbidden**: Insufficient permissions
- **404 Not Found**: Corpus not found
- **409 Conflict**: Document with same ID exists but different content
- **413 Payload Too Large**: File exceeds size limit

---

## Upload from file object (streaming)

<CodePanel
  title="Upload from file object (streaming)"
  snippets={[
    {
      language: 'python',
      code: `filename = "sample-security-policy.txt"

with open(filename, "rb") as file_obj:
    # No read() here - pass the file object directly for streaming
    response = client.upload.file(
        corpus_key="my-docs",
        file=file_obj,
        filename=filename,
        metadata={
            "document_type": "security_policy",
            "department": "security",
            "year": "2025",
            "classification": "internal"
        }
    )`
    }
  ]}
  annotations={{
    python: [
      { line: 3, text: 'Pass file object directly without reading into memory' },
      { line: 13, text: 'Classification metadata for precise filtering' }
    ]
  }}
  customWidth="50%"
/>

Upload files directly from file objects without loading the entire content into memory. 
This is ideal for streaming scenarios where files are large or come from dynamic sources 
like cloud storage (e.g., S3 downloads), APIs, or webhooks, avoiding memory overhead.

**Streaming Use Cases:**
- Files downloaded from cloud storage (S3, Google Cloud, etc.)
- Content received through APIs or webhooks
- Temporary files that don't need local persistence
- Batch processing from external systems

**Error Handling:**
- **400 Bad Request**: Invalid parameters or unsupported file type
- **403 Forbidden**: Insufficient permissions
- **404 Not Found**: Corpus not found
- **409 Conflict**: Document with same ID exists but different content
- **413 Payload Too Large**: File exceeds size limit


---

## Advanced upload with comprehensive metadata

<CodePanel
  title="Upload with comprehensive metadata and processing options"
  snippets={[
    {
      language: 'python',
      code: `filename = "sample-employee-handbook.txt" 
with open(filename, "rb") as f:
    file_content = f.read()

response = client.upload.file(
    corpus_key="my-docs",
    file=file_content,
    filename=filename,
    metadata={
        "document_type": "handbook",
        "department": "hr",
        "version": "2.1", 
        "effective_date": "2025-01-01",
        "review_date": "2026-01-01",
        "classification": "internal",
        "contains_policies": True,
        "language": "en"
    },
    table_extraction_config={"extract_tables": True},
    chunking_strategy={
        "type": "max_chars_chunking_strategy",
        "max_chars_per_chunk": 256
    }
)`
    }
  ]}
  annotations={{
    python: [
      { line: 9, text: 'Comprehensive metadata for better organization' },
      { line: 19, text: 'Enable extraction of any tables in the document' },
      { line: 22, text: 'Use smaller chunks for precise data retrieval' }
    ]
  }}
  customWidth="50%"
/>

Upload documents with comprehensive metadata to capture important business context. 
The system automatically processes document structure for 
precise queries and analysis.

**Comprehensive Metadata Benefits:**
- Better document organization and discovery
- Enhanced filtering capabilities in queries
- Support for compliance and audit requirements
- Improved search relevance through context

**Business Document Benefits:**
- Query by department: "Show all HR policies"
- Filter by dates: "Find documents effective after 2025"
- Search by classification: "Show internal documents only"
- Track versions: "Get the latest version of each handbook"

To update or overwrite an existing file, you must first delete the document 
using `client.documents.delete()` and then re-upload it, as direct updates to 
content are not supported. The file name is used as the document ID. 
Attempting to upload a file with the same name but different content 
will result in a 409 error.

**Error Handling:**
- **400 Bad Request**: Invalid parameters or unsupported file type
- **403 Forbidden**: Insufficient permissions
- **404 Not Found**: Corpus not found
- **409 Conflict**: Document with same ID exists but different content
- **413 Payload Too Large**: File exceeds size limit

<Spacer size="l" />

---

## Best practices and error handling

**File Upload Best Practices:**

<CodePanel
  title="Production-ready upload patterns"
  snippets={[
    {
      language: 'python',
      code: `def upload_file_safely(file_path, corpus_key, metadata=None):
    # Validate file exists and is readable
    if not os.path.exists(file_path):
        return {"success": False, "error": "File not found"}
    
    try:
        with open(file_path, "rb") as f:
            content = f.read()
        
        # Ensure file is not empty
        if len(content) == 0:
            return {"success": False, "error": "File is empty"}
        
        # Prepare metadata with defaults
        upload_metadata = {
            "upload_timestamp": "2025-07-03T12:00:00Z",
            "file_size_bytes": len(content),
            "original_filename": os.path.basename(file_path)
        }
        upload_metadata.update(metadata or {})
        
        # Upload with appropriate configuration
        response = client.upload.file(
            corpus_key=corpus_key,
            file=content,
            filename=os.path.basename(file_path),
            metadata=upload_metadata,
            table_extraction_config={"extract_tables": True}
        )
        
        return {"success": True, "response": response}
    
    except Exception as e:
        return {"success": False, "error": str(e)}`
    }
  ]}
  annotations={{
    python: [
      { line: 3, text: 'Validate file existence before attempting upload' },
      { line: 10, text: 'Check for empty files to avoid wasted API calls' },
      { line: 13, text: 'Add automatic metadata for tracking and auditing' },
      { line: 20, text: 'Enable table extraction by default for comprehensive indexing' }
    ]
  }}
  customWidth="50%"
/>

**Production Guidelines:**
- Always validate file existence and readability before upload
- Include comprehensive metadata for better searchability
- Use appropriate chunking strategies based on content type
- Enable table extraction for documents with structured data
- Implement retry logic for transient failures
- Monitor upload success rates and file processing times

**Error Handling:**
- **File Issues**: Validate file existence, permissions, and size
- **API Errors**: Check corpus permissions and file format support
- **Network Issues**: Implement retry logic with exponential backoff
- **Large Files**: Consider chunked uploads for very large documents

**Metadata Recommendations:**
- Include document type, department, and date information
- Add file size and upload timestamp for tracking
- Use consistent naming conventions across your organization
- Include business-specific fields for filtering and analytics

---

## Next steps

After understanding file uploads:

- **Query uploaded content**: Use [Queries](/docs/sdk/python/query) to search 
  uploaded documents.
- **Document management**: Use [Documents](/docs/sdk/python/documents) to manage 
  uploaded content.
- **Chat integration**: Build conversational interfaces with uploaded documents 
  using [Chats](/docs/sdk/python/chats).
